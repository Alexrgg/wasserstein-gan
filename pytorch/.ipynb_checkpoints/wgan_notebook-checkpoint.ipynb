{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  Referenced from: /Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantStub' from 'torch.ao.quantization' (/Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torch/ao/quantization/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mignite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events, Engine\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/models/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mswin_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaxvit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detection, optical_flow, quantization, segmentation, video\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model, get_model_builder, get_model_weights, get_weight, list_models\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/models/quantization/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgooglenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshufflenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/models/quantization/mobilenet.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401, F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401, F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __all__ \u001b[38;5;28;01mas\u001b[39;00m mv2_all\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/torchvision/models/quantization/mobilenetv2.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, Union\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeQuantStub, QuantStub\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertedResidual, MobileNet_V2_Weights, MobileNetV2\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuantStub' from 'torch.ao.quantization' (/Users/alejandrorodriguez/Library/Python/3.8/lib/python/site-packages/torch/ao/quantization/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import MNIST\n",
    "from ignite.engine.engine import Events, Engine\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "from model import get_critic, get_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    def __init__(self, data_loader):\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.data_loader.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iterator = iter(self.data_loader)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.data_loader)\n",
    "            batch = next(self.iterator)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class WGANTrainer:\n",
    "    def __init__(self, data_loader, gen: nn.Module, cri: nn.Module,\n",
    "                 opt_g, opt_c, n_cri, gp_lam: float, device):\n",
    "\n",
    "        assert len(n_cri) == 2\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.gen = gen\n",
    "        self.cri = cri\n",
    "        self.opt_g = opt_g\n",
    "        self.opt_c = opt_c\n",
    "        self.n_cri = n_cri\n",
    "        self.gp_lam = gp_lam\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, engine, batch):\n",
    "        # 複数回サンプリングする必要があるため渡されたbatchは使わない\n",
    "        # データはself.data_loaderから取得\n",
    "\n",
    "        self.gen.train()\n",
    "        self.cri.train()\n",
    "\n",
    "        # update critic\n",
    "        iter_num = engine.state.iteration\n",
    "        upd_num = self.n_cri[1] if iter_num <= 25 or iter_num % 500 == 0 else self.n_cri[0]\n",
    "        critic_mean_loss = 0\n",
    "        i = 0\n",
    "        for batch in self.data_loader:\n",
    "            x_real, _ = batch\n",
    "            x_real = x_real.to(self.device)\n",
    "\n",
    "            z = np.random.uniform(size=(self.data_loader.batch_size, Z_DIM)).astype(np.float32)\n",
    "            z = torch.from_numpy(z).to(self.device)\n",
    "            x_fake = self.gen(z).detach()\n",
    "            x_fake.requires_grad = True\n",
    "\n",
    "            cri_loss = (self.cri(x_fake) - self.cri(x_real)).mean()  # Wasserstein距離の逆符号\n",
    "\n",
    "            # gradient penalty\n",
    "            eps = np.random.uniform(size=(self.data_loader.batch_size, 1, 1, 1)).astype(np.float32)\n",
    "            eps = torch.from_numpy(eps).to(self.device)\n",
    "            x_fusion = eps * x_real + (1 - eps) * x_fake  # (N,1,H,W)\n",
    "            g_critic = torch.autograd.grad(torch.unbind(self.cri(x_fusion)), x_fusion, create_graph=True)[0]  # (N,1,H,W)\n",
    "            gp = g_critic.square().sum(dim=(1, 2, 3)).sqrt()\n",
    "            gp = (gp - 1).square().mean()\n",
    "\n",
    "            total_loss = cri_loss + self.gp_lam * gp\n",
    "\n",
    "            self.opt_c.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.opt_c.step()\n",
    "\n",
    "            critic_mean_loss += total_loss.item()\n",
    "\n",
    "            i += 1\n",
    "            if i >= upd_num:\n",
    "                break\n",
    "\n",
    "        critic_mean_loss /= upd_num\n",
    "\n",
    "        # update generator\n",
    "        z = np.random.uniform(size=(self.data_loader.batch_size, Z_DIM)).astype(np.float32)\n",
    "        z = torch.from_numpy(z).to(self.device)\n",
    "        x_fake = self.gen(z)\n",
    "        gen_loss = -self.cri(x_fake).mean()\n",
    "\n",
    "        self.opt_c.zero_grad()\n",
    "        self.opt_g.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        self.opt_g.step()\n",
    "\n",
    "        return {\n",
    "            \"critic_loss\": critic_mean_loss,\n",
    "            \"generator_loss\": gen_loss.item()\n",
    "        }\n",
    "\n",
    "\n",
    "class MetricsAccumulator:\n",
    "    def __init__(self, keys: list):\n",
    "        self.keys = keys\n",
    "        self.logs = {k: 0.0 for k in keys}\n",
    "        self.num = 0\n",
    "\n",
    "    def __call__(self, engine):\n",
    "        for k in self.keys:\n",
    "            self.logs[k] += engine.state.output[k]\n",
    "        self.num += 1\n",
    "\n",
    "    def get_mean(self, key):\n",
    "        return self.logs[key] / self.num\n",
    "\n",
    "    def reset(self):\n",
    "        self.logs = {k: 0.0 for k in self.keys}\n",
    "        self.num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_metrics(log_dict: dict, accumulator: MetricsAccumulator, keys: list = None):\n",
    "    if keys is None:\n",
    "        keys = accumulator.keys\n",
    "\n",
    "    def _record(engine):\n",
    "        _add(\"iteration\", engine.state.iteration)\n",
    "\n",
    "        for k in keys:\n",
    "            _add(k, accumulator.get_mean(k))\n",
    "\n",
    "        accumulator.reset()\n",
    "\n",
    "    def _add(key, value):\n",
    "        if key in log_dict:\n",
    "            log_dict[key].append(value)\n",
    "        else:\n",
    "            log_dict[key] = [value]\n",
    "\n",
    "    return _record\n",
    "\n",
    "\n",
    "def plot_metrics(log_dict: dict, x_key: str, y_keys: list, out_path: Path):\n",
    "    def _plot(engine):\n",
    "        plt.figure()\n",
    "        for k in y_keys:\n",
    "            plt.plot(log_dict[x_key], log_dict[k], label=k)\n",
    "        plt.legend()\n",
    "        plt.xlabel(x_key)\n",
    "        plt.savefig(str(out_path))\n",
    "        plt.close()\n",
    "\n",
    "    return _plot\n",
    "\n",
    "\n",
    "def print_metrics(log_dict: dict, keys: list):\n",
    "    return lambda engine: print(\", \".join(f\"{k}: {log_dict[k][-1]}\" for k in keys))\n",
    "\n",
    "\n",
    "def save_img(generator: nn.Module, out_dir_path: Path, device):\n",
    "    try:\n",
    "        out_dir_path.mkdir(parents=True)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    def _save(engine):\n",
    "        z = np.random.uniform(size=(1, Z_DIM)).astype(np.float32)\n",
    "        z = torch.from_numpy(z).to(device)\n",
    "\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            x = generator(z).detach().cpu().numpy().squeeze() * 255\n",
    "\n",
    "        p = out_dir_path / f\"out_iter_{engine.state.iteration:05d}.png\"\n",
    "        cv2.imwrite(str(p), x.astype(np.uint8))\n",
    "\n",
    "    return _save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"WGAN(-GP)\")\n",
    "    parser.add_argument(\"-b\", \"--batchsize\", type=int, default=100)\n",
    "    parser.add_argument(\"-i\", \"--iteration\", type=int, default=10000)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--beta1\", type=float, default=0)\n",
    "    parser.add_argument(\"--beta2\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--n_cri\", type=int, nargs=2, default=[5, 100])\n",
    "    parser.add_argument(\"--gp_lam\", type=float, default=10.0)\n",
    "    parser.add_argument(\"-g\", type=int, default=0, help=\"GPU ID (negative value indicates CPU mode)\")\n",
    "    parser.add_argument(\"--result_dir\", default=\"result\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    pprint.pprint(vars(args))\n",
    "    main(args)\n",
    "\n",
    "\n",
    "Z_DIM = 30\n",
    "def main(args):\n",
    "    if args.g >= 0 and torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{args.g:d}\")\n",
    "        print(f\"GPU mode: {args.g:d}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CPU mode\")\n",
    "\n",
    "    result_dir = Path(args.result_dir)\n",
    "\n",
    "    # MNISTデータ取得\n",
    "    mnist_train = MNIST(root=\".\", download=True, train=True,\n",
    "                        transform=lambda x: np.expand_dims(np.asarray(x, dtype=np.float32), 0) / 255)\n",
    "    mnist_train         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mnist_loader = DataLoader(mnist_train, args.batchsize)\n",
    "    mnist_loader = InfiniteDataLoader(mnist_loader)\n",
    "\n",
    "    generator = get_generator(Z_DIM).to(device)\n",
    "    critic = get_critic().to(device)\n",
    "\n",
    "    opt_g = Adam(generator.parameters(), args.alpha, (args.beta1, args.beta2))\n",
    "    opt_c = Adam(critic.parameters(), args.alpha, (args.beta1, args.beta2))\n",
    "\n",
    "    trainer = Engine(WGANTrainer(mnist_loader, generator, critic, opt_g, opt_c,\n",
    "                                 args.n_cri, args.gp_lam, device))\n",
    "\n",
    "    log_dict = {}\n",
    "    accumulator = MetricsAccumulator([\"generator_loss\", \"critic_loss\"])\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED, accumulator)\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED(every=500), record_metrics(log_dict, accumulator))\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED(every=500), print_metrics(log_dict, accumulator.keys))\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED(every=500),\n",
    "                              plot_metrics(log_dict, \"iteration\", accumulator.keys, result_dir / \"metrics.pdf\"))\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED(every=500), save_img(generator, result_dir / \"generated_samples\", device))\n",
    "\n",
    "    # 指定されたイテレーション数で終了させる\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED(once=args.iteration), lambda engine: engine.terminate())\n",
    "\n",
    "    trainer.run(mnist_loader, max_epochs=10**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_args()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
